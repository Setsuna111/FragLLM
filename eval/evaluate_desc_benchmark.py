"""
Single-thread script to compute metrics including BLEU, ROUGE and BERT scores.
Metrics will be computed on JSON files generated by `generate_instruct.py`.
The script is designed for single-GPU computation. 
"""

import argparse
import pandas as pd
import os
import re
from typing import Any, Dict, List

import evaluate
from transformers import BertTokenizer, RobertaTokenizer
import scripts.utils_argparse as utils_argparse


argParser = argparse.ArgumentParser()

argParser.add_argument("--results_path", type=str, help="path to save the generated description")
argParser.add_argument("--evaluate_exact_match", type=utils_argparse.str2bool)
argParser.add_argument("--evaluate_bleu", type=utils_argparse.str2bool)
argParser.add_argument("--evaluate_rouge", type=utils_argparse.str2bool)
argParser.add_argument("--evaluate_bert_score", type=utils_argparse.str2bool)
argParser.add_argument("--verbose", type=utils_argparse.str2bool)


def compute_exact_match(predictions: List[str], references: List[str]) -> float:
    """Compute exact match ratio allowing for case and punctuation differences."""
    def normalize(text: str) -> str:
        """Normalize text by lowercasing and removing punctuation."""
        text = text.lower()
        text = re.sub(r'[^\w]', '', text)
        return text

    exact_match = 0
    for pred, ref in zip(predictions, references):
        if normalize(pred) == normalize(ref):
            exact_match += 1
    return exact_match / len(predictions)


def compute_bleu2(predictions: List[str], references: List[str]) -> Dict[str, Any]:
    bleu = evaluate.load("./eval/metrics/bleu")
    return bleu.compute(predictions=predictions, references=references, max_order=2)


def compute_bleu4(predictions: List[str], references: List[str]) -> Dict[str, Any]: 
    bleu = evaluate.load("./eval/metrics/bleu")
    return bleu.compute(predictions=predictions, references=references)


def compute_rouge(predictions: List[str], references: List[str]) -> Dict[str, Any]: 
    rouge = evaluate.load("./eval/metrics/rouge")
    return rouge.compute(predictions=predictions, references=references)


def compute_bert_score(predictions: List[str], references: List[str]) -> Dict[str, Dict[str, Any]]: 
    """Compute BERT score on roberta-large and biobert-large respectively."""
    results: Dict[str, Dict[str, Any]] = {}

    tokenizer = RobertaTokenizer.from_pretrained("/home/djy/projects/Data/HF_models/roberta-large")
    retokenized_predictions = tokenizer(
        predictions, padding="max_length", truncation=True, max_length=495, return_tensors="pt"
    )["input_ids"]
    truncated_predictions = tokenizer.batch_decode(retokenized_predictions, skip_special_tokens=True)
    retokenized_labels = tokenizer(
        references, padding="max_length", truncation=True, max_length=495, return_tensors="pt"
    )["input_ids"]
    truncated_labels = tokenizer.batch_decode(retokenized_labels, skip_special_tokens=True)

    bert = evaluate.load("./eval/metrics/bertscore")
    roberta_results = bert.compute(predictions=truncated_predictions, references=truncated_labels, model_type="/home/djy/projects/Data/HF_models/roberta-large", num_layers=17)
    results["roberta-large"] = {
        "precision": sum(roberta_results["precision"]) / len(roberta_results["precision"]), 
        "recall": sum(roberta_results["recall"]) / len(roberta_results["recall"]), 
        "f1": sum(roberta_results["f1"]) / len(roberta_results["f1"])
    }

    # truncate sentences to fit max_position_embeddings=512 of biobert
    tokenizer = BertTokenizer.from_pretrained("/home/djy/projects/Data/HF_models/biobert-large-cased-v1.1")
    retokenized_predictions = tokenizer(
        predictions, padding="max_length", truncation=True, max_length=495, return_tensors="pt"
    )["input_ids"]
    truncated_predictions = tokenizer.batch_decode(retokenized_predictions, skip_special_tokens=True)
    retokenized_labels = tokenizer(
        references, padding="max_length", truncation=True, max_length=495, return_tensors="pt"
    )["input_ids"]
    truncated_labels = tokenizer.batch_decode(retokenized_labels, skip_special_tokens=True)

    biobert_results = bert.compute(
        predictions=truncated_predictions,
        references=truncated_labels,
        model_type="/home/djy/projects/Data/HF_models/biobert-large-cased-v1.1",
        num_layers=24,
    )
    results["biobert-large"] = {
        "precision": sum(biobert_results["precision"]) / len(biobert_results["precision"]), 
        "recall": sum(biobert_results["recall"]) / len(biobert_results["recall"]), 
        "f1": sum(biobert_results["f1"]) / len(biobert_results["f1"])
    }

    return results


def compute_metrics(predictions: List[str], references: List[str], args: Dict[str, Any]) -> Dict[str, Any]:
    """Compute BLEU, ROUGE, BERT scores and exact match ratio on given texts."""
    gathered_results: Dict[str, Dict[str, Any]] = {}

    if args["evaluate_exact_match"]:
        exact_match = compute_exact_match(predictions=predictions, references=references)
        gathered_results["exact_match"] = exact_match
        if args["verbose"]:
            print(f"EXACT match ratio: {exact_match}")

    if args["evaluate_bleu"]:
        bleu_results = compute_bleu2(predictions=predictions, references=references)
        gathered_results["bleu2"] = bleu_results
        if args["verbose"]:
            print(f"BLEU-2 score: {bleu_results}")
        bleu_results = compute_bleu4(predictions=predictions, references=references)
        gathered_results["bleu4"] = bleu_results
        if args["verbose"]:
            print(f"BLEU-4 score: {bleu_results}")

    if args["evaluate_rouge"]:
        rouge_results = compute_rouge(predictions=predictions, references=references)
        gathered_results["rouge"] = rouge_results
        if args["verbose"]:
            print(f"ROUGE score: {rouge_results}")
    
    if args["evaluate_bert_score"]:
        bert_results = compute_bert_score(predictions=predictions, references=references)
        gathered_results["bert"] = bert_results
        if args["verbose"]:
            for model_name, model_results in bert_results.items(): 
                print(f"BERT score with {model_name}: {model_results}")

    return gathered_results


def benchmark_csv(args: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
    """
    Evaluate generation results. 
    Compute metrics including BLEU, ROUGE and BERT scores and print results. 
    """
    res = pd.read_csv(args["results_path"])
    res = res.drop_duplicates()
    predictions = res['generated'].tolist()
    references = res['function'].tolist()
    return compute_metrics(predictions=predictions, references=references, args=args)


if __name__ == "__main__": 
    parsed_args = argParser.parse_args()

    print("####################")
    for key, value in parsed_args.__dict__.items(): 
        print(f"{key}: {value}")
    print("####################")
    # evaluate csv results
    benchmark_csv(parsed_args.__dict__)
