0905 lfj
New features
1. train_llama_lfj.py中新增loss检测代码，检测过大以及nan，追溯对应数据
2. models/protein_llama_lfj.py以及protein_arch_lfj.py中，新增使用门控机制融合global feature与residue feature的Q-former。

Bug fix
1. 修正ActRefDesc对data_name的定义

Others
1. dataset/dataloader_frag.py存在一些随机数bug的古战场遗迹，注意这些注释不要传到main，包括此条

0904 djy
1. 更新了requirements.text
2. README.md中添加了环境安装的流程
3. dataset/dataloader_frag.py中将HybridDataset中随机加载样本的方式更改为了根据索引加载样本

0904 lfj
1. 进一步修改了eval/evaluate_reference.py在多GPU并行时的数据问题。首先在dataset/dataloader_frag.py，dataset/dataloader_refferring.py，dataset/dataloader_function.py中添加了返回数据在数据集中dataset_idx的操作，随后于eval/evaluate_reference.py对多卡并行推理结果依据eval/evaluate_reference.py去重。
2. 由于测试数据集输出已经去重，故删除了evaluate_desc.py，evaluate_desc_benchmark.py中的去重代码，避免把（pred，label）相同的数据对剔除。
3. 修改了dataset/dataloader_refferring.py中对超长sequence的截断方式，当片段长度大于1021时，截断片段尾部；当序列长度大于1021，片段长度小于1021时，计算新序列起点的安全范围，保证片段不被截断，随后在安全范围内随机采样起点；当序列长度小于1021时，正常处理。
4. 删除了测试文件evaluate_reference_lfj.py，此外evaluate_function_desc.py/sh也可以删。

0903 lfj
1. 新增eval/evaluate_reference.py,eval/evaluate_reference.sh，合并prot2textinference任务的推理，支持单卡debug模式，并支持多文件评估。修改并行存储逻辑，解决了多卡并行存储时总数不对、保留列名重复。
2. trainer支持在training checkpoints时存储非lora参数，支持在resume_from_checkpoints时读入非lora参数，此处修改存储在train_llama_lfj.py，暂未修改到train_llama.py中
3. git merge使用说明：理想情况下，每一个开发分支只实现一个功能，开发结束后，及时合并入main，并checkout最新的main继续开发下一个功能。
4. 部分微信交换文件一并上传，包括：merge_lora_weights.py, merge_lora_weights_checkpoint.py, protein_arch.py对esm pooling layer的改动

0901_djy:修改内容
0. dataset/dataloader_frag.py中新增94-95，101-102，148-150行，适配description截断要求；
1. 调整models/protein_arch.py中182行FragmentAdapter初始化时，传入参数设错误的问题，没有config.adapter;
2. models/protein_llama.py中103行，将generate时prepare_inputs_labels_for_protein的无用传参设为None
3. merge_lora_weights.py 新增lora权重合并脚本
4. ./eval 新增description任务评估代码

0831 测试指标
1. ReferenceMetrics，统一的reference任务指标计算类，内含分类指标类和语言指标类
2. 支持多种输入
    1. Label-only: Traditional classification with pred_labels + target_labels
    2. InterPro text retrieval: LLM text descriptions + true InterPro IDs 
    3. Language metrics: Predicted texts + target texts for BLEU/ROUGE
    4. Combined: Any combination of the above for comprehensive evaluation

0830_djy:修改内容
0. protein_arch.py中调整了FragmentAdapter，统一了其调用过程，避免出现部分数据无需激活FragmentAdapter导致的计算图不一致的问题；
1. dataset/templates.py中扩充了referring和grounding任务的指令模板；
2. 额外添加了wandb/  sh_djy/  *.pyc  **/__pycache__/等文件的gitignore。


0830 修改+支持git
0. 支持使用git管理。0829云盘版本为初始main版本，再上传git之前，已通过compare folder同步所有修改。
1. 添加train_contrast_llama_allgather.py文件：
    （1）支持跨gpu对比学习
    （2）支持分minibatch推理
    （3）CosineAnnealing学习率衰减
    （4）对比策略改为取mean embedding
2. 修改模型参数加载：protein_arch.py line 199起两处，将keyword in k 修改为 '.' + keyword + '.' in k
3. git管理：
    （1）Ignored：/data /checkpoints /checkpoints_contrast /tensorboard_logs 这些文件需要本地复制或新建
    （2）若需要修改/尝试新东西，可以新建分支，确认尝试后merge到main中。若切换新服务器运行，记得pull一下main同步最新修改，科研助手上可能不是最新了
    （3）每次push具体的修改可以写到此文件中    


0829 修改
1. dataloader_grounding.py文件415行修改为 frags = data_item["fragments"]
2. scripts/train_llama.py 调整了一些参数的默认值（valid数据集默认为None）
3. models/protein_arch.py 226和227行调整为：
    inputs_embeds_old = self.get_model().get_input_embeddings()(input_ids)
    inputs_embeds = inputs_embeds_old.clone()
    避免lora训练时计算图出错
4.新增了scripts/fragllama_flash_attn_monkey_patch.py和scripts/train_mem.py，支持flashattention
5.stage2中batchsize调整为1（避免OOM）